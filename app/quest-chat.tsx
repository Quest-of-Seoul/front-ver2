import { Ionicons } from '@expo/vector-icons';
import { Audio } from 'expo-av';
import Constants from 'expo-constants';
import * as ImagePicker from 'expo-image-picker';
import { useRouter } from 'expo-router';
import * as Speech from 'expo-speech';
import { useEffect, useRef, useState } from 'react';
import {
  ActivityIndicator,
  Image,
  KeyboardAvoidingView,
  Modal,
  Platform,
  Pressable,
  ScrollView,
  StyleSheet,
  TextInput,
  View,
} from 'react-native';

import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { aiStationApi } from '@/services/api';
import { useQuestStore } from '@/store/useQuestStore';

const API_URL = Constants.expoConfig?.extra?.apiUrl || (Platform.OS === 'android' ? 'http://10.0.2.2:8000' : 'http://localhost:8000');
const makeId = () => `${Date.now()}-${Math.random().toString(16).slice(2)}`;

type Message = {
  id: string;
  role: 'assistant' | 'user';
  text?: string;
  imageUrl?: string;
};

type VLMContext = {
  placeName: string;
  description: string;
  vlmAnalysis?: string;
};

export default function QuestChatScreen() {
  const scrollRef = useRef<ScrollView>(null);
  const router = useRouter();
  const { activeQuest } = useQuestStore();

  // Access quest_id and place_id from active quest
  const questId = activeQuest?.quest_id;
  const placeId = activeQuest?.place_id;

  console.log('Quest Chat - Active Quest:', { questId, placeId });

  const [messages, setMessages] = useState<Message[]>([
    {
      id: makeId(),
      role: 'assistant',
      text: 'ì•ˆë…•í•˜ì„¸ìš”! ì„œìš¸ ê´€ê´‘ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”. ğŸ›ï¸\n\nì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ë©´ í•´ë‹¹ ì¥ì†Œë¥¼ ë¶„ì„í•´ë“œë¦´ê²Œìš”! ğŸ“¸',
    },
  ]);
  const [input, setInput] = useState('');
  const [showImageModal, setShowImageModal] = useState(false);
  const [vlmContext, setVlmContext] = useState<VLMContext | null>(null);
  const [isLoading, setIsLoading] = useState(false);
  const [showVoiceMode, setShowVoiceMode] = useState(false);
  const recordRef = useRef<Audio.Recording | null>(null);
  const [isRecording, setIsRecording] = useState(false);
  const [selectedImage, setSelectedImage] = useState<string | null>(null); // base64 ì´ë¯¸ì§€ ì €ì¥

  useEffect(() => {
    return () => {
      Speech.stop();
    };
  }, []);

  useEffect(() => {
    scrollRef.current?.scrollToEnd({ animated: true });
  }, [messages]);

  const addMessage = (msg: Message) => {
    setMessages((prev) => [...prev, msg]);
  };

  const handleImageSelected = async (base64img: string) => {
    // ì´ë¯¸ì§€ë¥¼ ì„ íƒí•˜ë©´ ë©”ì‹œì§€ë¡œ ì¶”ê°€í•˜ê³ , ì…ë ¥ì°½ì— í‘œì‹œ
    setSelectedImage(base64img);
    addMessage({
      id: makeId(),
      role: 'user',
      imageUrl: `data:image/jpeg;base64,${base64img}`,
    });
    // ì´ë¯¸ì§€ ì„ íƒ í›„ ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•  ìˆ˜ ìˆë„ë¡ ëŒ€ê¸°
  };

  const pickImageFromLibrary = async () => {
    setShowImageModal(false);
    const result = await ImagePicker.launchImageLibraryAsync({
      base64: true,
      quality: 0.8,
    });
    if (!result.canceled && result.assets?.[0].base64) {
      await handleImageSelected(result.assets[0].base64);
    }
  };

  const takePhoto = async () => {
    setShowImageModal(false);
    const permission = await ImagePicker.requestCameraPermissionsAsync();
    if (!permission.granted) return;
    const result = await ImagePicker.launchCameraAsync({
      base64: true,
      quality: 0.8,
    });
    if (!result.canceled && result.assets?.[0].base64) {
      await handleImageSelected(result.assets[0].base64);
    }
  };

  const analyzeImage = async (base64img: string, userMessage?: string) => {
    addMessage({
      id: makeId(),
      role: 'assistant',
      text: 'ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... ğŸ”',
    });
    try {
      // Quest ëª¨ë“œì¼ ë•ŒëŠ” quest VLM chat API ì‚¬ìš© (chat_logsì— ì €ì¥ë¨)
      if (questId) {
        console.log('Quest VLM Chat API:', `${API_URL}/ai-station/quest/vlm-chat`);
        console.log('Quest ID:', questId, 'Place ID:', placeId);

        const data = await aiStationApi.questVlmChat({
          image: base64img,
          user_message: userMessage || undefined,
          quest_id: questId,
          place_id: placeId ?? undefined,
          language: 'ko',
          prefer_url: true,
          enable_tts: false,
        });

        if (data?.message) {
          // VLM ì»¨í…ìŠ¤íŠ¸ ì €ì¥
          setVlmContext({
            placeName: data.place?.name || 'ì„œìš¸',
            description: data.message,
          });

          addMessage({
            id: makeId(),
            role: 'assistant',
            text: data.message,
          });

          // ì¥ì†Œ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì •ë³´ í‘œì‹œ
          if (data.place) {
            addMessage({
              id: makeId(),
              role: 'assistant',
              text: `ğŸ“ ${data.place.name || 'ì•Œ ìˆ˜ ì—†ëŠ” ì¥ì†Œ'}\n${data.place.address || ''}`,
            });
          }

          // í›„ì† ì§ˆë¬¸ ì•ˆë‚´
          addMessage({
            id: makeId(),
            role: 'assistant',
            text: 'ì´ ì¥ì†Œì— ëŒ€í•´ ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ’¬',
          });
        } else {
          addMessage({
            id: makeId(),
            role: 'assistant',
            text: 'ë¶„ì„ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ì—ˆì–´ìš”.',
          });
        }
      } else {
        // Explore ëª¨ë“œì¼ ë•ŒëŠ” ê¸°ì¡´ VLM analyze API ì‚¬ìš©
        console.log('VLM API URL:', `${API_URL}/vlm/analyze`);

        const data = await aiStationApi.vlmAnalyze({
          image: base64img,
          language: 'ko',
          prefer_url: true,
          enable_tts: false,
        });

        if (data?.description) {
          // VLM ì»¨í…ìŠ¤íŠ¸ ì €ì¥
          setVlmContext({
            placeName: data.place?.name || 'ì„œìš¸',
            description: data.description,
            vlmAnalysis: data.vlm_analysis,
          });

          addMessage({
            id: makeId(),
            role: 'assistant',
            text: data.description,
          });

          // ì¥ì†Œ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì •ë³´ í‘œì‹œ
          if (data.place) {
            addMessage({
              id: makeId(),
              role: 'assistant',
              text: `ğŸ“ ${data.place.name || 'ì•Œ ìˆ˜ ì—†ëŠ” ì¥ì†Œ'}\n${data.place.address || ''}`,
            });
          }

          // í›„ì† ì§ˆë¬¸ ì•ˆë‚´
          addMessage({
            id: makeId(),
            role: 'assistant',
            text: 'ì´ ì¥ì†Œì— ëŒ€í•´ ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ’¬',
          });
        } else {
          addMessage({
            id: makeId(),
            role: 'assistant',
            text: 'ë¶„ì„ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ì—ˆì–´ìš”.',
          });
        }
      }
    } catch (error) {
      console.error('VLM analyze error:', error);
      addMessage({
        id: makeId(),
        role: 'assistant',
        text: 'ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
      });
    }
  };

  const sendMessage = async () => {
    // ì´ë¯¸ì§€ê°€ ì„ íƒë˜ì–´ ìˆìœ¼ë©´ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤í–‰
    if (selectedImage) {
      const userText = input.trim();
      if (userText) {
        addMessage({
          id: makeId(),
          role: 'user',
          text: userText,
        });
      }
      setInput('');
      setIsLoading(true);
      await analyzeImage(selectedImage, userText || undefined);
      setSelectedImage(null);
      setIsLoading(false);
      return;
    }

    // ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ í…ìŠ¤íŠ¸ ë©”ì‹œì§€
    if (!input.trim() || isLoading) return;

    const userText = input.trim();
    addMessage({
      id: makeId(),
      role: 'user',
      text: userText,
    });
    setInput('');
    setIsLoading(true);

    try {
      // VLM ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ í¬í•¨, ì—†ìœ¼ë©´ ì¼ë°˜ ëŒ€í™”
      let requestBody;

      if (vlmContext) {
        const contextMessage = `[ì´ì „ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼]
${vlmContext.description}

[ì‚¬ìš©ì ì§ˆë¬¸]
${userText}`;

        requestBody = {
          landmark: vlmContext.placeName,
          user_message: contextMessage,
          language: 'ko',
          prefer_url: true,
          enable_tts: false,
          quest_id: questId, // í€˜ìŠ¤íŠ¸ ID í¬í•¨
          place_id: placeId ?? undefined, // ì¥ì†Œ ID í¬í•¨
        };
      } else {
        // VLM ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ì„œìš¸ ê´€ê´‘ ëŒ€í™”
        requestBody = {
          landmark: 'ì„œìš¸',
          user_message: userText,
          language: 'ko',
          prefer_url: true,
          enable_tts: false,
          quest_id: questId, // í€˜ìŠ¤íŠ¸ ID í¬í•¨
          place_id: placeId ?? undefined, // ì¥ì†Œ ID í¬í•¨
        };
      }

      const data = await aiStationApi.docentChat(requestBody);

      addMessage({
        id: makeId(),
        role: 'assistant',
        text: data.message || 'ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.',
      });
    } catch (error) {
      console.error('Chat error:', error);
      addMessage({
        id: makeId(),
        role: 'assistant',
        text: 'ì‘ë‹µì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
      });
    } finally {
      setIsLoading(false);
    }
  };

  const exitToPrevious = () => {
    router.back();
  };

  // === STT + TTS ===
  const startRecording = async () => {
    try {
      await Audio.requestPermissionsAsync();
      await Audio.setAudioModeAsync({
        allowsRecordingIOS: true,
        playsInSilentModeIOS: true,
      });

      const recording = new Audio.Recording();
      await recording.prepareToRecordAsync(
        Audio.RecordingOptionsPresets.HIGH_QUALITY
      );
      await recording.startAsync();

      recordRef.current = recording;
      setIsRecording(true);
      console.log("ë…¹ìŒ ì‹œì‘");
    } catch (err) {
      console.error("ë…¹ìŒ ì‹¤íŒ¨:", err);
    }
  };

  const stopRecording = async () => {
    try {
      const recording = recordRef.current;
      if (!recording) return null;

      await recording.stopAndUnloadAsync();
      const uri = recording.getURI();
      setIsRecording(false);

      if (!uri) return null;

      const fileData = await fetch(uri);
      const blob = await fileData.blob();

      const reader = new FileReader();
      return new Promise<string>((resolve) => {
        reader.onloadend = () => {
          const base64 = (reader.result as string).split(',')[1];
          resolve(base64);
        };
        reader.readAsDataURL(blob);
      });
    } catch (err) {
      console.error("ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨:", err);
      setIsRecording(false);
      return null;
    }
  };

  const runSTTandTTS = async () => {
    try {
      const base64Audio = await stopRecording();
      if (!base64Audio) return;

      console.log("STT ìš”ì²­ ì¤‘...");

      const data = await aiStationApi.sttTts({
        audio: base64Audio,
        language_code: "ko-KR",
        prefer_url: false,
      });

      const text = data.transcribed_text;

      // 1) í…ìŠ¤íŠ¸ë¥¼ ìœ ì € ì±„íŒ…ìœ¼ë¡œ ì¶”ê°€
      const msg: Message = {
        id: makeId(),
        role: "user",
        text: text,
      };
      addMessage(msg);

      // 2) ì´ì–´ì„œ ê¸°ì¡´ RAG Chatìœ¼ë¡œ ìš”ì²­
      await sendMessageFromSTT(text);

      // 3) TTS ì¬ìƒ (optional)
      if (data.audio) {
        const sound = new Audio.Sound();
        await sound.loadAsync({ uri: `data:audio/mp3;base64,${data.audio}` });
        await sound.playAsync();
      }

    } catch (e) {
      console.error("STT/TTS ì˜¤ë¥˜:", e);
    }
  };

  const sendMessageFromSTT = async (text: string) => {
    setIsLoading(true);
    try {
      // VLM ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ í¬í•¨, ì—†ìœ¼ë©´ ì¼ë°˜ ëŒ€í™”
      let requestBody;

      if (vlmContext) {
        const contextMessage = `[ì´ì „ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼]
${vlmContext.description}

[ì‚¬ìš©ì ì§ˆë¬¸]
${text}`;

        requestBody = {
          landmark: vlmContext.placeName,
          user_message: contextMessage,
          language: 'ko',
          prefer_url: true,
          enable_tts: false,
          quest_id: questId, // í€˜ìŠ¤íŠ¸ ID í¬í•¨
          place_id: placeId ?? undefined, // ì¥ì†Œ ID í¬í•¨
        };
      } else {
        // VLM ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ì„œìš¸ ê´€ê´‘ ëŒ€í™”
        requestBody = {
          landmark: 'ì„œìš¸',
          user_message: text,
          language: 'ko',
          prefer_url: true,
          enable_tts: false,
          quest_id: questId, // í€˜ìŠ¤íŠ¸ ID í¬í•¨
          place_id: placeId ?? undefined, // ì¥ì†Œ ID í¬í•¨
        };
      }

      const data = await aiStationApi.docentChat(requestBody);

      addMessage({
        id: makeId(),
        role: 'assistant',
        text: data.message || 'ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.',
      });
    } catch (err) {
      console.error('STT Chat error:', err);
      addMessage({
        id: makeId(),
        role: 'assistant',
        text: 'ì‘ë‹µì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
      });
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <KeyboardAvoidingView behavior={Platform.OS === 'ios' ? 'padding' : undefined} style={{ flex: 1 }}>
      <ThemedView style={styles.container}>
        <View style={styles.header}>
          <View style={styles.headerRow}>
            <ThemedText type="title">Quest Chat</ThemedText>
            <Pressable onPress={exitToPrevious} style={styles.closeButton}>
              <Ionicons name="close" size={20} color="#fff" />
            </Pressable>
          </View>
          <ThemedText>AI Docentê³¼ ëŒ€í™”ë¥¼ ë‚˜ëˆ ë³´ì„¸ìš”.</ThemedText>
        </View>

        <ScrollView ref={scrollRef} style={{ flex: 1 }} contentContainerStyle={styles.messages}>
          {messages.map((msg) => (
            <View
              key={msg.id}
              style={[styles.bubble, msg.role === 'assistant' ? styles.assistantBubble : styles.userBubble]}
            >
              {msg.text && (
                <ThemedText style={msg.role === 'user' ? styles.userText : undefined}>{msg.text}</ThemedText>
              )}
              {msg.imageUrl && (
                <Image source={{ uri: msg.imageUrl }} style={{ width: 180, height: 180, borderRadius: 12, marginTop: 6 }} />
              )}
            </View>
          ))}
        </ScrollView>

        {/* ì„ íƒëœ ì´ë¯¸ì§€ í”„ë¦¬ë·° */}
        {selectedImage && (
          <View style={styles.imagePreviewContainer}>
            <Image
              source={{ uri: `data:image/jpeg;base64,${selectedImage}` }}
              style={styles.imagePreview}
              resizeMode="cover"
            />
            <Pressable
              style={styles.removeImageButton}
              onPress={() => setSelectedImage(null)}
            >
              <Ionicons name="close-circle" size={24} color="#fff" />
            </Pressable>
            <ThemedText style={styles.imagePreviewText}>
              {input.trim() ? 'ë©”ì‹œì§€ì™€ í•¨ê»˜ ì „ì†¡' : 'ì´ë¯¸ì§€ë§Œ ì „ì†¡í•˜ë ¤ë©´ ì—”í„°ë¥¼ ëˆ„ë¥´ì„¸ìš”'}
            </ThemedText>
          </View>
        )}

        <View style={styles.inputRow}>
          <Pressable
            style={[styles.photoButton, isLoading && styles.buttonDisabled]}
            onPress={() => setShowImageModal(true)}
            disabled={isLoading}
          >
            <Ionicons name="image-outline" size={22} color="#fff" />
          </Pressable>
          <TextInput
            style={styles.input}
            placeholder={selectedImage ? "ì¶”ê°€ ë©”ì‹œì§€ ì…ë ¥ (ì„ íƒ)" : "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”"}
            placeholderTextColor="#7a7a7a"
            value={input}
            onChangeText={setInput}
            editable={!isLoading}
            onSubmitEditing={sendMessage}
          />
          <Pressable
            style={[styles.sendButton, isLoading && styles.buttonDisabled]}
            onPress={sendMessage}
            disabled={isLoading || (!input.trim() && !selectedImage)}
          >
            {isLoading ? (
              <ActivityIndicator color="#fff" size="small" />
            ) : (
              <Ionicons name="paper-plane" size={20} color="#fff" />
            )}
          </Pressable>
          <Pressable
            style={styles.voiceButton}
            onPress={() => setShowVoiceMode(true)}
          >
            <Ionicons name="ellipse" size={22} color="#fff" />
          </Pressable>
        </View>

        <Modal visible={showImageModal} transparent animationType="slide" onRequestClose={() => setShowImageModal(false)}>
          <View style={styles.modalOverlay}>
            <View style={styles.modalBox}>
              <Pressable style={styles.modalItem} onPress={takePhoto}>
                <Ionicons name="camera" size={20} color="#111" />
                <ThemedText style={styles.modalText}>ì‚¬ì§„ ì°ê¸°</ThemedText>
              </Pressable>
              <Pressable style={styles.modalItem} onPress={pickImageFromLibrary}>
                <Ionicons name="image" size={20} color="#111" />
                <ThemedText style={styles.modalText}>ì•¨ë²”ì—ì„œ ì„ íƒ</ThemedText>
              </Pressable>
              <Pressable style={styles.modalCancel} onPress={() => setShowImageModal(false)}>
                <ThemedText style={styles.modalCancelText}>ì·¨ì†Œ</ThemedText>
              </Pressable>
            </View>
          </View>
        </Modal>

        {showVoiceMode && (
          <VoiceModeOverlay
            onClose={() => setShowVoiceMode(false)}
            isRecording={isRecording}
            onStartRecording={startRecording}
            onStopRecording={async () => {
              await runSTTandTTS();
              recordRef.current = null;
              setShowVoiceMode(false);
            }}
          />
        )}
      </ThemedView>
    </KeyboardAvoidingView>
  );
}

type VoiceModeOverlayProps = {
  onClose: () => void;
  isRecording: boolean;
  onStartRecording: () => void;
  onStopRecording: () => void;
};

function VoiceModeOverlay({ onClose, isRecording, onStartRecording, onStopRecording }: VoiceModeOverlayProps) {
  return (
    <View style={overlayStyles.overlay}>
      <View style={[overlayStyles.circle, isRecording && overlayStyles.circleRecording]} />
      <View style={overlayStyles.bottomMenu}>
        <Pressable style={overlayStyles.menuButton}>
          <Ionicons name="videocam-outline" size={30} color="#aaa" />
        </Pressable>
        <Pressable
          style={[overlayStyles.menuButton, isRecording && overlayStyles.menuButtonRecording]}
          onPress={async () => {
            if (!isRecording) {
              await onStartRecording();
            } else {
              await onStopRecording();
            }
          }}
        >
          <Ionicons name="mic" size={30} color="#fff" />
        </Pressable>
        <Pressable style={overlayStyles.menuButton}>
          <Ionicons name="ellipsis-horizontal" size={30} color="#aaa" />
        </Pressable>
        <Pressable style={overlayStyles.menuButton} onPress={onClose}>
          <Ionicons name="close" size={34} color="#fff" />
        </Pressable>
      </View>
    </View>
  );
}

const overlayStyles = StyleSheet.create({
  overlay: {
    position: 'absolute',
    top: 0,
    left: 0,
    right: 0,
    bottom: 0,
    backgroundColor: '#000',
    justifyContent: 'center',
    alignItems: 'center',
    zIndex: 999,
  },
  circle: {
    width: 120,
    height: 120,
    borderRadius: 60,
    backgroundColor: '#fff',
    marginBottom: 200,
  },
  circleRecording: {
    backgroundColor: '#FF4444',
  },
  bottomMenu: {
    position: 'absolute',
    bottom: 50,
    width: '100%',
    flexDirection: 'row',
    justifyContent: 'space-around',
    paddingHorizontal: 20,
  },
  menuButton: {
    width: 60,
    height: 60,
    borderRadius: 30,
    backgroundColor: '#222',
    justifyContent: 'center',
    alignItems: 'center',
  },
  menuButtonRecording: {
    backgroundColor: '#FF4444',
  },
});

const styles = StyleSheet.create({
  container: {
    flex: 1,
    paddingTop: 60,
    paddingHorizontal: 20,
    paddingBottom: 24,
  },
  header: {
    marginBottom: 12,
  },
  headerRow: {
    flexDirection: 'row',
    alignItems: 'center',
    justifyContent: 'space-between',
    marginBottom: 4,
  },
  closeButton: {
    width: 32,
    height: 32,
    borderRadius: 16,
    backgroundColor: 'rgba(255,255,255,0.15)',
    alignItems: 'center',
    justifyContent: 'center',
  },
  messages: {
    paddingVertical: 20,
    gap: 10,
  },
  bubble: {
    maxWidth: '80%',
    padding: 12,
    borderRadius: 14,
    marginBottom: 10,
  },
  assistantBubble: {
    alignSelf: 'flex-start',
    backgroundColor: '#C1C9D9',
  },
  userBubble: {
    alignSelf: 'flex-end',
    backgroundColor: '#5B7DFF',
  },
  userText: {
    color: '#fff',
  },
  inputRow: {
    flexDirection: 'row',
    alignItems: 'center',
    gap: 10,
  },
  photoButton: {
    width: 46,
    height: 46,
    borderRadius: 14,
    backgroundColor: '#64748B',
    alignItems: 'center',
    justifyContent: 'center',
  },
  input: {
    flex: 1,
    borderRadius: 14,
    backgroundColor: '#E2E8F0',
    paddingHorizontal: 14,
    paddingVertical: 10,
  },
  sendButton: {
    width: 46,
    height: 46,
    borderRadius: 14,
    backgroundColor: '#5B7DFF',
    alignItems: 'center',
    justifyContent: 'center',
  },
  voiceButton: {
    width: 46,
    height: 46,
    borderRadius: 23,
    backgroundColor: '#64748B',
    justifyContent: 'center',
    alignItems: 'center',
  },
  buttonDisabled: {
    opacity: 0.6,
  },
  imagePreviewContainer: {
    position: 'relative',
    marginBottom: 10,
    padding: 10,
    backgroundColor: '#1E293B',
    borderRadius: 12,
  },
  imagePreview: {
    width: '100%',
    height: 150,
    borderRadius: 8,
    marginBottom: 8,
  },
  removeImageButton: {
    position: 'absolute',
    top: 15,
    right: 15,
    backgroundColor: 'rgba(0,0,0,0.5)',
    borderRadius: 12,
  },
  imagePreviewText: {
    fontSize: 12,
    color: '#94A3B8',
    textAlign: 'center',
  },
  modalOverlay: {
    flex: 1,
    justifyContent: 'flex-end',
    backgroundColor: 'rgba(0,0,0,0.45)',
  },
  modalBox: {
    backgroundColor: '#fff',
    padding: 20,
    borderTopLeftRadius: 24,
    borderTopRightRadius: 24,
    gap: 18,
  },
  modalItem: {
    flexDirection: 'row',
    alignItems: 'center',
  },
  modalText: {
    marginLeft: 8,
  },
  modalCancel: {
    alignItems: 'center',
    marginTop: 8,
  },
  modalCancelText: {
    color: '#777',
  },
});


